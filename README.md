# SDE-Project_3-Behavioral-Cloning-Project

## Introdution

This is 3rd project in self driving car course. The objective of this project is to clone human behavior using Deep Neural Network to effectively teach a car to drive autonomously. This project use Udacity's simulator, you can find it [here](https://github.com/udacity/self-driving-car-sim) 

In training mode, inputs are the data generated by user, the camera images and control data(steering angle, throttle, brake, speed). Those inputs data go through keras CNN model(`model.py`) and produce output (prediction steering angle) and saved as `model.json`(with trained weight saved as `model.h5`)

For people who want to run the project without training, you can run the command as `python drive.py model.jason`, the model weights are retrieved using the same name but with the extension `.h5`

If you want to training the model by yourself, please make sure you have GPU. And collect enough data through simulator, usually more than two track.

### Dependencies
* Tensorflow 2.0
* keras
* Numpy
* SciPy
* OpenCV
* Pandas




## Approach 

### Model detail

In project, I chose the NVIDIA model as our CNN model (see below), because this structure has been identified to be a good model to start with. The model is from this [paper](chrome-extension://gphandlahdpffmccakmbngmbjnjiiahp/https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) 

![](https://raw.githubusercontent.com/jeremy-shannon/CarND-Behavioral-Cloning-Project/master/images/nVidia_model.png)

```python
def build_model(args):
    """
    NVIDIA model used
    Image normalization to avoid saturation and make gradients work better.
    Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU
    Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU
    Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU
    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
    Drop out (0.5)
    Fully connected: neurons: 100, activation: ELU
    Fully connected: neurons: 50, activation: ELU
    Fully connected: neurons: 10, activation: ELU
    Fully connected: neurons: 1 (output)
    # the convolution layers are meant to handle feature engineering
    the fully connected layer for predicting the steering angle.
    dropout avoids overfitting
    ELU(Exponential linear unit) function takes care of the Vanishing gradient problem.
    """
    model = Sequential()
    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))
    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))
    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))
    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))
    model.add(Conv2D(64, 3, 3, activation='elu'))
    model.add(Conv2D(64, 3, 3, activation='elu'))
    model.add(Dropout(args.keep_prob))
    model.add(Flatten())
    model.add(Dense(100, activation='elu'))
    model.add(Dense(50, activation='elu'))
    model.add(Dense(10, activation='elu'))
    model.add(Dense(1))
    model.summary()

    return model
```
#### Regression network and loss function

This is model is to predict steering angle, so the output is just a single output node. For loss function is use mean squared error(MSE), because the main purpose of our model is to minimize the steering error between predicts and ground truth. 
```python
model.compile(loss='mean_squared_error', optimizer=Adam(lr=args.learning_rate))
```
>Note: **Cross-entropy loss** measure the performance of a classification model whose output is probability value between 0 and 1, so it perferred for **classification**, while MSE is one of the best choices for regression

#### Preprocessing data
* Normalizing the data
* Mean centering the data
```python
# add Lambda function 
# dividing each element by 127.5, all the data are normalized to 0-2
# Then mean center the image by subtracting 1 from each element
model.add(Lambda(lambda x: x/127.5-1.0, input_shape = INPUT_SHAPE))
```

#### Data augmentation 

Data augmentation will help the model generalize better. More data for training the network, the data we use for training the network is more comprehensive. 
There are many ways to do augmentation:
* Channge the brightness on the images  
* Shift Horizontally or vertically
* Flip the images horizontally and invert the steering angles, so they are still banlanced data. 
* Crop images 

#### Visualizing Loss and Generators
```python
    # Fits the model on data generated batch-by-batch by a Python generator.
    # The generator is run in parallel to the model, for space efficiency.
    # For instance, this allows you to do real-time data augmentation on images on CPU in
    # parallel to training your model on GPU.
    # so we reshape our data into their appropriate batches and train our model simulatenously
    model.fit_generator(batch_generator(args.data_dir, X_train, y_train, args.batch_size, True),
                        args.samples_per_epoch,
                        args.nb_epoch,
                        max_q_size=1,
                        validation_data=batch_generator(args.data_dir, X_valid, y_valid, args.batch_size, False),
                        nb_val_samples=len(X_valid),
                        callbacks=[checkpoint],
                        verbose=1)
```
`verbose = 1`: 
* Output a progress bar in the terminal as the model trains
* Output loss metric on the training set as the model trains
* Output the loss on the training and validation sets after each epoch

`verbose = 2`: Keras will only output the loss on the training set and validation set after each epoch. 

## Summary 
Here are the parameters this model need to update.  

|Layer              |Output Shape      | Param    |
|:---:              |:------------:    | :---:    |
|Cropping2D Layer   | (None, 65,320,3) |   0      |
|Lambda Layer       | (None, 65,320,3) |   0      |
|Conv Layer 1       | (None, 31,158,24)|   1824   |
|Conv Layer 2       | (None, 14,77,36) |   21636  |
|Conv Layer 3       | (None, 5,37,48)  |   43248  |
|Conv Layer 4       | (None, 3,35,64)  |   27712  |
|Conv Layer 5       | (None, 1,33,64)  |   36928  |
|Dropout Layer 1    | (None, 1,33,64)  |   0      |
|Flatten Layer 1    | (None,2112)      |   0      |
|Dense Layer 1      | (None,100)       |   211300 |
|Dense Layer 2      | (None, 50)       |   5050   |
|Dense Layer 3      | (None, 10)       |   510    |
|Dense Layer 4      | (None, 1)        |   11     |

> Note: here are some tips for who want to run the project themselves. 
* Drive during trianing mode for more than 3 laps to collect more data. 
* Trianing with GPU
* I found a [video](https://www.youtube.com/watch?v=EaY5QiZwSP4&t=900s) may help you to understand. 

